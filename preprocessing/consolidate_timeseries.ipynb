{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating data\n",
    "\n",
    "This file consists of functions that consolidate our disparate datasets into one large dataset that is useful in training our model. \n",
    "\n",
    "The goal is to generate a file with 30 columns (this number should be variable), such that each column is a state in time. \n",
    "\n",
    "Ideally, this will be done with heirachical data, ie `p1` is the first point in time, and within `p1` you have an x component, y component, etc.\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/advanced.html\n",
    "\n",
    "## Input data format\n",
    "\n",
    "It is assumed that the input data with have the columns: `[timestamp,tx,ty,tz,qx,qy,qz,qw]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data we want\n",
    "\n",
    "This function will create velocity and acceleration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features(raw: pd.DataFrame, dropna: bool = False) -> None:\n",
    "    raw['vx'] = raw['tx'].diff() / raw['timestamp'].diff()\n",
    "    raw['vy'] = raw['ty'].diff() / raw['timestamp'].diff()\n",
    "    raw['vz'] = raw['tz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    raw['ax'] = raw['vx'].diff() / raw['timestamp'].diff()\n",
    "    raw['ay'] = raw['vy'].diff() / raw['timestamp'].diff()\n",
    "    raw['az'] = raw['vz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    if dropna: raw.dropna(inplace=True)\n",
    "\n",
    "# test the above functions\n",
    "\n",
    "# df = pd.read_csv(\"../data/fpv_uzh/indoor_forward_3_davis_with_gt.txt\")\n",
    "# extract_features(df, dropna=True)\n",
    "\n",
    "# print(df.head())\n",
    "# print(df['timestamp'])\n",
    "\n",
    "# trial = df[0:4] \n",
    "# trial.head()\n",
    "\n",
    "# temp = np.DataFrame.arange(4)\n",
    "# print(temp)\n",
    "# trial.loc[:, \"trajNum\"] = np.arange(len(trial))\n",
    "# trial.loc[:, \"slice\"] = 1\n",
    "# trial.head()\n",
    "\n",
    "# trial.set_index(['slice', 'trajNum'], inplace=True)\n",
    "# trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the data\n",
    "\n",
    "Now, we want rows of data that represent a specific range of time. In this case, we want 30 points for each new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slices(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    cols = [f\"{col}_{i}\" for i in range(n) for col in data.columns]\n",
    "    slices = []\n",
    "    for i in range(len(data) - n):\n",
    "        flattened = pd.DataFrame([data[i:i+n].to_numpy().flatten()])\n",
    "        flattened.columns = cols\n",
    "        slices.append(flattened)\n",
    "    return pd.concat(slices, ignore_index=False)\n",
    "\n",
    "# test the above function for 4 pints in each row\n",
    "\n",
    "# slices = generate_slices(df, 4)\n",
    "# # print(slices.head())\n",
    "# slices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiIndex(data: pd.DataFrame, n: int, filename: str) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    slices = []\n",
    "    # i is the number of slices we want\n",
    "    for i in range(len(data) - n):\n",
    "        trial = data.copy()[i:i+n]\n",
    "        trial.loc[:, \"trajNum\"] = np.arange(len(trial))\n",
    "        trial.loc[:, \"slice\"] = filename + \":\" + str(i)\n",
    "        trial.set_index(['slice', 'trajNum'], inplace=True)\n",
    "        slices.append(trial)\n",
    "    return pd.concat(slices, ignore_index=False)\n",
    "\n",
    "# test\n",
    "# slices = multiIndex(df, 30, 0)\n",
    "# # print(slices.head())\n",
    "# print(slices.head(40))\n",
    "# type(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate all our original data\n",
    "\n",
    "Now, we want to consolidate our data from all the other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1d3f5edc85d9437887dd4210203393c3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/5093 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['timestamp', 'tx', 'ty', 'tz'],\n",
      "        num_rows: 766140\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "No objects to concatenate",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 16\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(output_path):\n\u001b[1;32m     12\u001b[0m     os\u001b[38;5;241m.\u001b[39mmakedirs(output_path)\n\u001b[0;32m---> 16\u001b[0m consolidated \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat(slices, ignore_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m     17\u001b[0m consolidated\u001b[38;5;241m.\u001b[39mto_csv(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconsolidated.csv\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.13/site-packages/pandas/core/reshape/concat.py:382\u001b[0m, in \u001b[0;36mconcat\u001b[0;34m(objs, axis, join, ignore_index, keys, levels, names, verify_integrity, sort, copy)\u001b[0m\n\u001b[1;32m    379\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m copy \u001b[38;5;129;01mand\u001b[39;00m using_copy_on_write():\n\u001b[1;32m    380\u001b[0m     copy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m--> 382\u001b[0m op \u001b[38;5;241m=\u001b[39m _Concatenator(\n\u001b[1;32m    383\u001b[0m     objs,\n\u001b[1;32m    384\u001b[0m     axis\u001b[38;5;241m=\u001b[39maxis,\n\u001b[1;32m    385\u001b[0m     ignore_index\u001b[38;5;241m=\u001b[39mignore_index,\n\u001b[1;32m    386\u001b[0m     join\u001b[38;5;241m=\u001b[39mjoin,\n\u001b[1;32m    387\u001b[0m     keys\u001b[38;5;241m=\u001b[39mkeys,\n\u001b[1;32m    388\u001b[0m     levels\u001b[38;5;241m=\u001b[39mlevels,\n\u001b[1;32m    389\u001b[0m     names\u001b[38;5;241m=\u001b[39mnames,\n\u001b[1;32m    390\u001b[0m     verify_integrity\u001b[38;5;241m=\u001b[39mverify_integrity,\n\u001b[1;32m    391\u001b[0m     copy\u001b[38;5;241m=\u001b[39mcopy,\n\u001b[1;32m    392\u001b[0m     sort\u001b[38;5;241m=\u001b[39msort,\n\u001b[1;32m    393\u001b[0m )\n\u001b[1;32m    395\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.13/site-packages/pandas/core/reshape/concat.py:445\u001b[0m, in \u001b[0;36m_Concatenator.__init__\u001b[0;34m(self, objs, axis, join, keys, levels, names, ignore_index, verify_integrity, copy, sort)\u001b[0m\n\u001b[1;32m    442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverify_integrity \u001b[38;5;241m=\u001b[39m verify_integrity\n\u001b[1;32m    443\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcopy \u001b[38;5;241m=\u001b[39m copy\n\u001b[0;32m--> 445\u001b[0m objs, keys \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_clean_keys_and_objs(objs, keys)\n\u001b[1;32m    447\u001b[0m \u001b[38;5;66;03m# figure out what our result ndim is going to be\u001b[39;00m\n\u001b[1;32m    448\u001b[0m ndims \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_ndims(objs)\n",
      "File \u001b[0;32m~/miniconda3/envs/ml/lib/python3.13/site-packages/pandas/core/reshape/concat.py:507\u001b[0m, in \u001b[0;36m_Concatenator._clean_keys_and_objs\u001b[0;34m(self, objs, keys)\u001b[0m\n\u001b[1;32m    504\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(objs)\n\u001b[1;32m    506\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(objs_list) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 507\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo objects to concatenate\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m keys \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    510\u001b[0m     objs_list \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(com\u001b[38;5;241m.\u001b[39mnot_none(\u001b[38;5;241m*\u001b[39mobjs_list))\n",
      "\u001b[0;31mValueError\u001b[0m: No objects to concatenate"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"riotu-lab/Synthetic-UAV-Flight-Trajectories\")\n",
    "print(ds)\n",
    "\n",
    "n = 30\n",
    "is_multi_index = True\n",
    "slices = []\n",
    "output_path = \"../data/output\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "\n",
    "\n",
    "consolidated = pd.concat(slices, ignore_index=False)\n",
    "consolidated.to_csv(os.path.join(output_path, \"consolidated.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
