{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Consolidating data\n",
    "\n",
    "This file consists of functions that consolidate our disparate datasets into one large dataset that is useful in training our model. \n",
    "\n",
    "The goal is to generate a file with 30 columns (this number should be variable), such that each column is a state in time. \n",
    "\n",
    "Ideally, this will be done with heirachical data, ie `p1` is the first point in time, and within `p1` you have an x component, y component, etc.\n",
    "\n",
    "https://pandas.pydata.org/docs/user_guide/advanced.html\n",
    "\n",
    "## Input data format\n",
    "\n",
    "It is assumed that the input data with have the columns: `[timestamp,tx,ty,tz,qx,qy,qz,qw]`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting the data we want\n",
    "\n",
    "This function will create velocity and acceleration columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def extract_features(raw: pd.DataFrame, dropna: bool = False) -> None:\n",
    "    raw['vx'] = raw['tx'].diff() / raw['timestamp'].diff()\n",
    "    raw['vy'] = raw['ty'].diff() / raw['timestamp'].diff()\n",
    "    raw['vz'] = raw['tz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    raw['ax'] = raw['vx'].diff() / raw['timestamp'].diff()\n",
    "    raw['ay'] = raw['vy'].diff() / raw['timestamp'].diff()\n",
    "    raw['az'] = raw['vz'].diff() / raw['timestamp'].diff()\n",
    "\n",
    "    if dropna: raw.dropna(inplace=True)\n",
    "\n",
    "# test the above functions\n",
    "\n",
    "# df = pd.read_csv(\"../data/fpv_uzh/indoor_forward_3_davis_with_gt.txt\")\n",
    "# extract_features(df, dropna=True)\n",
    "\n",
    "# print(df.head())\n",
    "# print(df['timestamp'])\n",
    "\n",
    "# trial = df[0:4] \n",
    "# trial.head()\n",
    "\n",
    "# temp = np.DataFrame.arange(4)\n",
    "# print(temp)\n",
    "# trial.loc[:, \"trajNum\"] = np.arange(len(trial))\n",
    "# trial.loc[:, \"slice\"] = 1\n",
    "# trial.head()\n",
    "\n",
    "# trial.set_index(['slice', 'trajNum'], inplace=True)\n",
    "# trial.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Slicing the data\n",
    "\n",
    "Now, we want rows of data that represent a specific range of time. In this case, we want 30 points for each new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_slices(data: pd.DataFrame, n: int) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    cols = [f\"{col}_{i}\" for i in range(n) for col in data.columns]\n",
    "    slices = []\n",
    "    for i in range(len(data) - n):\n",
    "        flattened = pd.DataFrame([data[i:i+n].to_numpy().flatten()])\n",
    "        flattened.columns = cols\n",
    "        slices.append(flattened)\n",
    "    return pd.concat(slices, ignore_index=False)\n",
    "\n",
    "# test the above function for 4 pints in each row\n",
    "\n",
    "# slices = generate_slices(df, 4)\n",
    "# # print(slices.head())\n",
    "# slices.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiIndex(data: pd.DataFrame, n: int, filename: str) -> pd.DataFrame:\n",
    "    # each row in the original data is a \"point\". Each row in the output \n",
    "    # is a list of points of size n. \n",
    "    slices = []\n",
    "    # i is the number of slices we want\n",
    "    for i in range(len(data) - n):\n",
    "        trial = data.copy()[i:i+n]\n",
    "        trial.loc[:, \"trajNum\"] = np.arange(len(trial))\n",
    "        trial.loc[:, \"slice\"] = filename + \":\" + str(i)\n",
    "        trial.set_index(['slice', 'trajNum'], inplace=True)\n",
    "        slices.append(trial)\n",
    "    return pd.concat(slices, ignore_index=False)\n",
    "\n",
    "# test\n",
    "# slices = multiIndex(df, 30, 0)\n",
    "# # print(slices.head())\n",
    "# print(slices.head(40))\n",
    "# type(slices)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Consolidate all our original data\n",
    "\n",
    "Now, we want to consolidate our data from all the other sources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "../data ['random_trajectory_100ms', 'fpv_uzh'] []\n",
      "../data/random_trajectory_100ms [] ['circle_31.txt', 'lemniscate_41.txt']\n",
      "../data/fpv_uzh [] ['indoor_forward_7_davis_with_gt.txt', 'indoor_forward_6_davis_with_gt.txt', 'indoor_forward_3_davis_with_gt.txt', 'indoor_forward_5_davis_with_gt.txt', 'indoor_forward_10_davis_with_gt.txt', 'indoor_forward_9_snapdragon_with_gt.txt', 'indoor_forward_3_snapdragon_with_gt.txt', 'indoor_forward_7_snapdragon_with_gt.txt', 'indoor_forward_10_snapdragon_with_gt.txt', 'indoor_forward_9_davis_with_gt.txt', 'indoor_forward_6_snapdragon_with_gt.txt', 'indoor_forward_5_snapdragon_with_gt.txt']\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "n = 30\n",
    "is_multi_index = True\n",
    "slices = []\n",
    "output_path = \"../data/output\"\n",
    "if not os.path.exists(output_path):\n",
    "    os.makedirs(output_path)\n",
    "\n",
    "# recursively get all text files in the data directory\n",
    "for dirpath, dirnames, filenames in os.walk(\"../data\", topdown=True):\n",
    "\n",
    "    # do not walk down  output dir\n",
    "    if 'output' in dirnames:\n",
    "        dirnames.remove('output')\n",
    "\n",
    "    print(dirpath, dirnames, filenames)\n",
    "\n",
    "    for filename in filenames:\n",
    "        if filename[-3:] == \"txt\":\n",
    "            filepath = os.path.join(dirpath, filename)\n",
    "            df = pd.read_csv(filepath)\n",
    "            \n",
    "            extract_features(df)\n",
    "            if is_multi_index:\n",
    "                slices.append(multiIndex(df, n, filename))\n",
    "            else:\n",
    "                slices.append(generate_slices(df, n))\n",
    "\n",
    "consolidated = pd.concat(slices, ignore_index=False)\n",
    "consolidated.to_csv(os.path.join(output_path, \"consolidated.csv\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
