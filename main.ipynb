{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "40b5ab04",
   "metadata": {},
   "source": [
    "# Main\n",
    "\n",
    "This file contains everything you need to run the model. This requires already be in csv format in the \"data/clean\" directory.\n",
    "\n",
    "You must run everything in \"scripts\" before running this file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b014d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e94ba6",
   "metadata": {},
   "source": [
    "## Split data\n",
    "\n",
    "5-fold stratified cross validation. Must be stratified because training data comes from different distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e550d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from math import floor\n",
    "import random\n",
    "from typing import TypedDict\n",
    "\n",
    "\n",
    "class Fold(TypedDict):\n",
    "    train: list[str]\n",
    "    validation: list[str]\n",
    "    test: list[str]\n",
    "        \n",
    "root = \"data/velocity/raw\"\n",
    "strata = os.listdir(root)\n",
    "k: int = 5\n",
    "shuffle = False\n",
    "\n",
    "folds: list[Fold] = [\n",
    "    {\"train\": [], \"validation\": [], \"test\": []}\n",
    "    for _ in range(k)\n",
    "]\n",
    "# Assume all csv's have unique names\n",
    "for stratum in strata:\n",
    "    files = os.listdir(os.path.join(root, stratum))\n",
    "    if shuffle:\n",
    "        random.shuffle(files)\n",
    "\n",
    "    m = len(files)\n",
    "    \n",
    "    for i in range(k):\n",
    "        fold_start = floor(m * (i/k))\n",
    "        fold_end = floor(m * ((i+1)/k))\n",
    "        \n",
    "        fold = files[fold_start:fold_end]\n",
    "\n",
    "        fold_train = floor(len(fold) * 0.65)\n",
    "        fold_validation = floor(len(fold) * 0.85)\n",
    "\n",
    "        folds[i][\"train\"].extend([os.path.join(root, stratum, f) for f in fold[0 : fold_train]])\n",
    "        folds[i][\"validation\"].extend([os.path.join(root, stratum, f) for f in fold[fold_train : fold_validation]])\n",
    "        folds[i][\"test\"].extend([os.path.join(root, stratum, f) for  f in fold[fold_validation :]])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8f3472",
   "metadata": {},
   "source": [
    "### This class defines our custom trajectory dataset\n",
    "\n",
    "I optimized it in the most practical way, balancing ram usage and conversion overheads."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb27408",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrajectoryDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self, files: list[str], input_length: int, output_length: int):\n",
    "        self.input_length = input_length\n",
    "        self.output_length = output_length\n",
    "        self.total_sequence_length = input_length + output_length\n",
    "\n",
    "        self.sample_map: list[tuple[int, int, int]] = []\n",
    "        \n",
    "        self.data_arrays: list[np.ndarray] = [] \n",
    "        \n",
    "        current_global_index = 0\n",
    "\n",
    "        for df_idx, file in enumerate(files):\n",
    "            try:\n",
    "                df: pd.DataFrame = pd.read_csv(file, usecols=[\"vx\", \"vy\", \"vz\"])\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}. Skipping.\")\n",
    "                continue\n",
    "\n",
    "            numeric_cols = df.select_dtypes(include=['number']).columns\n",
    "            if numeric_cols.empty:\n",
    "                print(f\"{file} has no numeric columns. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            data_array = df[numeric_cols].values.astype(np.float32) # Ensure float32 here\n",
    "            \n",
    "            if len(data_array) < self.total_sequence_length:\n",
    "                print(f\"{file} is too short ({len(data_array)} rows) for input_length={input_length} and output_length={output_length}. Skipping.\")\n",
    "                continue\n",
    "            \n",
    "            num_sequences_in_df = len(data_array) - self.total_sequence_length + 1\n",
    "            \n",
    "            for i in range(num_sequences_in_df):\n",
    "                self.sample_map.append((current_global_index + i, df_idx, i))\n",
    "            \n",
    "            current_global_index += num_sequences_in_df\n",
    "            self.data_arrays.append(data_array) # Store the NumPy array\n",
    "\n",
    "        self.total_samples = current_global_index\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return self.total_samples\n",
    "\n",
    "    def __getitem__(self, index: int) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        if not (0 <= index < self.total_samples):\n",
    "            raise IndexError(f\"Index {index} is out of bounds for dataset of size {self.total_samples}\")\n",
    "\n",
    "        global_index, df_idx, local_start_row = self.sample_map[index]\n",
    "\n",
    "        data_array = self.data_arrays[df_idx] # Retrieve NumPy array\n",
    "\n",
    "        x_start_local = local_start_row\n",
    "        x_end_local = local_start_row + self.input_length\n",
    "\n",
    "        y_start_local = x_end_local\n",
    "        y_end_local = y_start_local + self.output_length\n",
    "        \n",
    "        # Slice NumPy arrays (very fast)\n",
    "        x_data = data_array[x_start_local:x_end_local]\n",
    "        y_data = data_array[y_start_local:y_end_local]\n",
    "        \n",
    "        # Convert slices to PyTorch tensors (still happens in __getitem__, but from NumPy)\n",
    "        # This conversion is very efficient from NumPy arrays\n",
    "        x_tensor = torch.from_numpy(x_data) \n",
    "        y_tensor = torch.from_numpy(y_data)\n",
    "        \n",
    "        return x_tensor, y_tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62243e2",
   "metadata": {},
   "source": [
    "### Our Model\n",
    "\n",
    "Currently a basic GRU encoder-decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5adf80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TrajectoryPredictor(nn.Module):\n",
    "    \"\"\"\n",
    "    An Encoder-Decoder model for trajectory prediction using GRU units.\n",
    "    It takes an input sequence of points and predicts a future sequence of points.\n",
    "    \"\"\"\n",
    "    def __init__(self, \n",
    "                 input_features_dim: int, \n",
    "                 hidden_state_dim: int, \n",
    "                 output_features_dim: int, \n",
    "                 num_gru_layers: int,\n",
    "                 prediction_sequence_length: int):\n",
    "        \"\"\"\n",
    "        Initializes the TrajectoryPredictor model.\n",
    "\n",
    "        Args:\n",
    "            input_features_dim (int): The number of features in each input time step\n",
    "                                      (e.g., 2 for (x,y) coordinates).\n",
    "            hidden_state_dim (int): The number of features in the hidden state of the GRU layers.\n",
    "                                    This also determines the dimensionality of the context vector.\n",
    "            output_features_dim (int): The number of features to predict at each output time step.\n",
    "                                       (e.g., 2 for (x,y) coordinates).\n",
    "            num_gru_layers (int): The number of stacked GRU layers for both encoder and decoder.\n",
    "            prediction_sequence_length (int): The fixed number of future time steps to predict.\n",
    "        \"\"\"\n",
    "        super().__init__() # Cleaner way to call super() in Python 3+\n",
    "\n",
    "        self.hidden_state_dim = hidden_state_dim\n",
    "        self.num_gru_layers = num_gru_layers\n",
    "        self.prediction_sequence_length = prediction_sequence_length\n",
    "        self.output_features_dim = output_features_dim\n",
    "\n",
    "        self.encoder_gru = nn.GRU(input_features_dim, hidden_state_dim, num_gru_layers, batch_first=True)\n",
    "        \n",
    "        self.decoder_gru = nn.GRU(hidden_state_dim, hidden_state_dim, num_gru_layers, batch_first=True)\n",
    "        \n",
    "        self.output_projection_layer = nn.Linear(hidden_state_dim, output_features_dim)\n",
    "\n",
    "    def forward(self, input_sequence: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"\n",
    "        Performs the forward pass of the TrajectoryPredictor.\n",
    "\n",
    "        Args:\n",
    "            input_sequence (torch.Tensor): The input trajectory sequence.\n",
    "                                           Expected shape: (batch_size, input_seq_len, input_features_dim)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The predicted future trajectory sequence.\n",
    "                          Expected shape: (batch_size, prediction_sequence_length, output_features_dim)\n",
    "        \"\"\"\n",
    "        device = input_sequence.device\n",
    "\n",
    "        encoder_outputs, encoder_final_hidden_state = self.encoder_gru(input_sequence) \n",
    "\n",
    "        decoder_input_sequence = torch.zeros(\n",
    "            input_sequence.size(0), \n",
    "            self.prediction_sequence_length, \n",
    "            self.hidden_state_dim \n",
    "        ).to(device) \n",
    "\n",
    "        decoder_outputs, _ = self.decoder_gru(decoder_input_sequence, encoder_final_hidden_state) \n",
    "        predicted_trajectory = self.output_projection_layer(decoder_outputs) \n",
    "        \n",
    "        return predicted_trajectory"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc44fd5",
   "metadata": {},
   "source": [
    "### Basic training\n",
    "\n",
    "Actual training will happen in another script due to limitations of slurm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a08a33f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "\n",
    "fold = folds[0]\n",
    "\n",
    "X_len, y_len = 20, 10\n",
    "train_dataset = TrajectoryDataset(fold['train'], X_len, y_len)\n",
    "validation_dataset = TrajectoryDataset(fold['validation'], X_len, y_len)\n",
    "test_dataset = TrajectoryDataset(fold['test'], X_len, y_len)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=10)\n",
    "validation_loader = torch.utils.data.DataLoader(validation_dataset, batch_size=10)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=10)\n",
    "\n",
    "model = TrajectoryPredictor(\n",
    "    input_features_dim=3,\n",
    "    hidden_state_dim=64,\n",
    "    output_features_dim=3,\n",
    "    num_gru_layers=2,\n",
    "    prediction_sequence_length=y_len\n",
    ")\n",
    "\n",
    "loss_fn = torch.nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "def train_one_epoch(epoch_index, tb_writer):\n",
    "    running_loss = 0.\n",
    "    last_loss = 0.\n",
    "\n",
    "    # Here, we use enumerate(training_loader) instead of\n",
    "    # iter(training_loader) so that we can track the batch\n",
    "    # index and do some intra-epoch reporting\n",
    "    for i, data in tqdm(enumerate(train_loader), \"train_dataset\"):\n",
    "        # Every data instance is an input + label pair\n",
    "        inputs, labels = data\n",
    "\n",
    "        # Zero your gradients for every batch!\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Make predictions for this batch\n",
    "        outputs = model(inputs)\n",
    "\n",
    "        # Compute the loss and its gradients\n",
    "        loss = loss_fn(outputs, labels)\n",
    "        loss.backward()\n",
    "\n",
    "        # Adjust learning weights\n",
    "        optimizer.step()\n",
    "\n",
    "        # Gather data and report\n",
    "        running_loss += loss.item()\n",
    "        if i % 1000 == 999:\n",
    "            last_loss = running_loss / 1000 # loss per batch\n",
    "            print('  batch {} loss: {}'.format(i + 1, last_loss))\n",
    "            tb_x = epoch_index * len(train_loader) + i + 1\n",
    "            tb_writer.add_scalar(\"Loss/train\", last_loss, tb_x)\n",
    "            running_loss = 0.\n",
    "\n",
    "    return last_loss\n",
    "\n",
    "# Initializing in a separate cell so we can easily add more epochs to the same run\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "writer = SummaryWriter(\"runs/afrl_trainer_{}\".format(timestamp))\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "best_vloss = 1_000_000.\n",
    "\n",
    "for epoch in tqdm(range(EPOCHS), \"epoch\"):\n",
    "    # Make sure gradient tracking is on, and do a pass over the data\n",
    "    model.train(True)\n",
    "    avg_loss = train_one_epoch(epoch, writer)\n",
    "\n",
    "\n",
    "    running_vloss = 0.0\n",
    "    # Set the model to evaluation mode, disabling dropout and using population\n",
    "    # statistics for batch normalization.\n",
    "    model.eval()\n",
    "\n",
    "    # Disable gradient computation and reduce memory consumption.\n",
    "    with torch.no_grad():\n",
    "        for i, vdata in enumerate(validation_loader):\n",
    "            vinputs, vlabels = vdata\n",
    "            voutputs = model(vinputs)\n",
    "            vloss = loss_fn(voutputs, vlabels)\n",
    "            running_vloss += vloss\n",
    "\n",
    "    avg_vloss = running_vloss / (i + 1)\n",
    "    print(timestamp, 'LOSS train {} valid {}'.format(avg_loss, avg_vloss))\n",
    "\n",
    "    # Log the running loss averaged per batch\n",
    "    # for both training and validation\n",
    "    writer.add_scalars('Training vs. Validation Loss',\n",
    "                    { 'Training' : avg_loss, 'Validation' : avg_vloss },\n",
    "                    epoch + 1)\n",
    "    writer.flush()\n",
    "\n",
    "    # Track best performance, and save the model's state\n",
    "    if avg_vloss < best_vloss:\n",
    "        best_vloss = avg_vloss\n",
    "        model_path = 'model_{}_{}'.format(timestamp, epoch)\n",
    "        torch.save(model.state_dict, model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "afrl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
