\documentclass{article}
\usepackage{amsmath}
\begin{document}

\title{
    Exploring Deep Learning Techniques for Long Term Compute-Constrained UAV Trajectory Prediction
}

\author{
    Shokoufeh Mirzaei
    \and Sam Ly
    \and Anna Chiu
    \and Megan Bee
    \and Ray Tan
    \and Sidd Raj
}

\date{\today}

\maketitle

\begin{abstract}
    The ability to predict the flight paths of UAV in real-time dynamic 
    environments accurately is critical to many applications in commercial and 
    defense environments. UAV flight dynamics are uniquely hard to model due to 
    their non-linear nature, and the numerous unaccountable variables. 
    Furthermore, the influence of the pilot introduces a non-deterministic 
    element to UAV flight paths. Consequently, most  modern trajectory 
    prediction models rely on data-driven techniques to overcome these 
    challenges. In our research, we explore deep learning techniques, taking 
    advantage of an encoder-decoder architecture along with various neural 
    networks for performing sequence to sequence prediction. 
\end{abstract}

% \noindent\textbf{Keywords:} Deep Learning, Neural Networks, Drones, Transformers, LSTM, Informer, Encoder-decoder

\section{Introduction}
\subsection{Problem Statement}
The problem of "UAV trajectory prediction" can be formalized as finding a function $F$ 
that maps a list of length $n$ of \textbf{recorded points} $X$ to a list of 
length $m$ of \textbf{predicted points} $\hat{Y}$. The values $n$ and $m$ are the 
\textbf{input sequence length} and \textbf{output sequence length} respectively.

Since we are performing this task in 3D space, each \textbf{point} ($x_i$ and 
$\hat{y_i}$) is actually a vector in $R^3$. Thus,
$$ X = \{x_1, x_2, ..., x_n\}, \hat{Y} = \{\hat{y}_1, \hat{y}_2, ..., \hat{y}_m\} $$ 
$$ x_i \in R^3 \text{, for } i \in [1, n]$$
$$\hat{y}_j \in R^3 \text{, for } j \in [1, m]$$
and,
$$ \hat{Y} = F(X) $$

We can also see that $X$ and $\hat{Y}$ can also be thought of as an $n \times 3$
matrix and $m \times 3$ matrix respectively. Thus our final definition becomes:
$$ F: R^{n \times 3} \to R^{m \times 3} $$
Viewing $X$ and $\hat{Y}$ as a list of points is more intuitive, but viewing 
them as matrices allows us to more clearly visualize computations.

Note that our definition of the input sequence $X$ and output sequence $\hat{Y}$ 
does not include any information about the time. Instead, the time is encoded
\textbf{implicity} via the ordering of points $x_i$. Consequently, our model
expects the timestep to remain constant. Other models may use time as a feature
of the input sequence. (citation needed)

We can then vary the prediction length by varying $n$ and $m$. 

Since we are using machine learning approach, our model $F$ is parameterized by
$\theta$, denoted as $F_{\theta}$. We must define a loss function $L$
between our prediction $\hat{Y}$, and the actual future path $Y$. We choose to 
define $L$ as a function of the \textbf{model parameters} $\theta$. Thus, our 
\textbf{optimal model parameters} $\theta^*$ is the set of model parameters that 
minimizes our loss $L$. (cite ian goodfellow)
$$ \theta^* = \underset{\theta}{\text{arg min}}\  L(\theta)$$
Our theorectically optimal model is then denoted as $F_{\theta^*}$.

Ultimately, our problem boils down to a \textbf{sequence to sequence} prediction, 
of which many machine learning techniques have been devised, namely:
\begin{itemize}
    \item {
        \textbf{Encoder-decoder architectures} with various neural networks 
        (RNN, CNN, GRU, etc.)
    }
    \item {
        \textbf{LSTM} models
    }
    \item {
        \textbf{Transformer} models
    }
    \item {
        \textbf{Informer} models
    }
\end{itemize}
Our choice of seq2seq model is discussed later in this paper.

\section{Methods}
\section{Results}
\section{Discussion}
\end{document}