{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4502215d",
   "metadata": {},
   "source": [
    "# Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b7927fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def get_n_element_slices(dataframe: pd.DataFrame, n: int) -> list[pd.DataFrame]:\n",
    "    if not isinstance(dataframe, pd.DataFrame):\n",
    "        raise TypeError(\"Input must be a pandas DataFrame.\")\n",
    "    if not isinstance(n, int) or n <= 0:\n",
    "        raise ValueError(\"n must be a positive integer.\")\n",
    "    if n > len(dataframe):\n",
    "        return [] # No possible slices of length n if n is larger than the DataFrame\n",
    "\n",
    "    slices = []\n",
    "    for i in range(len(dataframe) - n + 1):\n",
    "        slice_df = dataframe.iloc[i : i + n].copy()\n",
    "        regularize_timestamp(slice_df)\n",
    "        slices.append(slice_df)\n",
    "    return slices\n",
    "\n",
    "# reset start to zero\n",
    "# TODO: \n",
    "def regularize_timestamp(df: pd.DataFrame):\n",
    "    start = df.iloc[0, 0]\n",
    "    df[\"timestamp\"] = df[\"timestamp\"] - start\n",
    "\n",
    "\n",
    "all_slices = {}\n",
    "\n",
    "data_path = \"../data/clean\"\n",
    "for filename in os.listdir(data_path):\n",
    "    if filename.endswith(\".csv\"):\n",
    "        df = pd.read_csv(os.path.join(data_path, filename), \n",
    "                         sep=' ',\n",
    "                         comment=\"#\", \n",
    "                         header=None,\n",
    "                         names=[\"timestamp\", \"tx\", \"ty\", \"tz\", \"qx\", \"qy\", \"qz\", \"qw\"])\n",
    "\n",
    "        slices = get_n_element_slices(df, 200)\n",
    "        all_slices[filename] = slices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9b3c5ac0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total slices:  306316\n",
      "Train slices:  0.68\n",
      "indoor_forward_6_snapdragon_with_gt.csv 14901\n",
      "indoor_forward_5_snapdragon_with_gt.csv 9451\n",
      "indoor_forward_7_snapdragon_with_gt.csv 33151\n",
      "indoor_45_12_snapdragon_with_gt.csv 19951\n",
      "indoor_forward_9_snapdragon_with_gt.csv 14201\n",
      "indoor_45_9_snapdragon_with_gt.csv 10501\n",
      "indoor_forward_10_snapdragon_with_gt.csv 14751\n",
      "outdoor_forward_1_snapdragon_with_gt.csv 21101\n",
      "indoor_45_14_snapdragon_with_gt.csv 18301\n",
      "indoor_45_2_snapdragon_with_gt.csv 24501\n",
      "outdoor_forward_5_snapdragon_with_gt.csv 8601\n",
      "indoor_45_13_snapdragon_with_gt.csv 18501\n",
      "Val slices:  0.15\n",
      "indoor_45_4_snapdragon_with_gt.csv 21401\n",
      "indoor_forward_3_snapdragon_with_gt.csv 24551\n",
      "Test slices:  0.17\n",
      "outdoor_45_1_snapdragon_with_gt.csv 9801\n",
      "outdoor_forward_3_snapdragon_with_gt.csv 42651\n"
     ]
    }
   ],
   "source": [
    "total_slices = 0\n",
    "for name, slices in all_slices.items():\n",
    "    curr_slices =len(slices) \n",
    "    total_slices += curr_slices\n",
    "\n",
    "print(\"Total slices: \", total_slices)\n",
    "# Prevent data leakage by spliting by path\n",
    "test_slices = 0\n",
    "test_set = {}\n",
    "val_slices = 0\n",
    "val_set = {}\n",
    "train_slices = 0\n",
    "train_set = {}\n",
    "for name, slices in all_slices.items():\n",
    "    if test_slices / total_slices < 0.15:\n",
    "        test_slices += len(slices)\n",
    "        test_set[name] = slices\n",
    "    elif val_slices / total_slices < 0.15:\n",
    "        val_slices += len(slices)\n",
    "        val_set[name] = slices\n",
    "    else:\n",
    "        train_slices += len(slices)\n",
    "        train_set[name] = slices\n",
    "\n",
    "print(\"Train slices: \", round(train_slices / total_slices, 2))\n",
    "for name, slices in train_set.items():\n",
    "    print(name, len(slices))\n",
    "print(\"Val slices: \", round(val_slices / total_slices, 2))\n",
    "for name, slices in val_set.items():\n",
    "    print(name, len(slices))\n",
    "print(\"Test slices: \", round(test_slices / total_slices, 2))\n",
    "for name, slices in test_set.items():\n",
    "    print(name, len(slices))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c892236",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "class TrajectoryDataset(Dataset):\n",
    "    def __init__(self, slices):\n",
    "        # print(slices)\n",
    "        self.data = torch.tensor(np.array(slices))\n",
    "            \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # X, y\n",
    "        return self.data[idx, 0:len(self.data)-1], self.data[idx, len(self.data)-1]\n",
    "\n",
    "train_loader = DataLoader(TrajectoryDataset([x for lst in train_set.values() for x in lst]), batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(TrajectoryDataset([x for lst in val_set.values() for x in lst]), batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(TrajectoryDataset([x for lst in test_set.values() for x in lst]), batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88060ce6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
